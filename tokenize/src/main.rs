//! 形態素解析を実行するユーティリティ
//!
//! このバイナリは、標準入力から読み込んだテキストを形態素解析し、
//! 指定された出力形式（mecab、wakati、detail）で結果を出力します。

use std::error::Error;
use std::io::{BufRead, BufWriter, Write};
use std::path::PathBuf;
use std::str::FromStr;

use vibrato_rkyv::dictionary::Dictionary;
use vibrato_rkyv::{CacheStrategy, Tokenizer};

use clap::Parser;

/// 出力モード
#[derive(Clone, Debug)]
enum OutputMode {
    Mecab,
    Wakati,
    Detail,
}

/// `OutputMode` の `FromStr` 実装
impl FromStr for OutputMode {
    type Err = &'static str;

    /// 文字列から出力モードをパースする
    ///
    /// # 引数
    ///
    /// * `mode` - パース対象の文字列（"mecab"、"wakati"、"detail"のいずれか）
    ///
    /// # 戻り値
    ///
    /// パースに成功した場合は対応する `OutputMode`、失敗した場合はエラーメッセージ
    fn from_str(mode: &str) -> Result<Self, Self::Err> {
        match mode {
            "mecab" => Ok(Self::Mecab),
            "wakati" => Ok(Self::Wakati),
            "detail" => Ok(Self::Detail),
            _ => Err("Could not parse a mode"),
        }
    }
}

/// コマンドライン引数
#[derive(Parser, Debug)]
#[clap(name = "tokenize", about = "Predicts morphemes")]
struct Args {
    /// System dictionary (in zstd).
    #[clap(short = 'i', long)]
    sysdic: PathBuf,

    /// Output mode. Choices are mecab, wakati, and detail.
    #[clap(short = 'O', long, default_value = "mecab")]
    output_mode: OutputMode,

    /// Ignores white spaces in input strings.
    #[clap(short = 'S', long)]
    ignore_space: bool,

    /// Maximum length of unknown words.
    #[clap(short = 'M', long)]
    max_grouping_len: Option<usize>,
}

/// メイン関数
///
/// 辞書をロードし、標準入力から読み込んだテキストを形態素解析して、
/// 指定された形式で結果を標準出力に出力します。
///
/// # 戻り値
///
/// 実行が成功した場合は `Ok(())`、エラーが発生した場合はエラー情報
fn main() -> Result<(), Box<dyn Error>> {
    let args = Args::parse();

    eprintln!("Loading the dictionary...");
    let dict = Dictionary::from_zstd(args.sysdic, CacheStrategy::GlobalCache)?;

    let tokenizer = Tokenizer::new(dict)
        .ignore_space(args.ignore_space)?
        .max_grouping_len(args.max_grouping_len.unwrap_or(0));
    let mut worker = tokenizer.new_worker();

    eprintln!("Ready to tokenize");

    let is_tty = atty::is(atty::Stream::Stdout);

    let out = std::io::stdout();
    let mut out = BufWriter::new(out.lock());
    let lines = std::io::stdin().lock().lines();
    for line in lines {
        let line = line?;
        worker.reset_sentence(line);
        worker.tokenize();
        match args.output_mode {
            OutputMode::Mecab => {
                for i in 0..worker.num_tokens() {
                    let t = worker.token(i);
                    out.write_all(t.surface().as_bytes())?;
                    out.write_all(b"\t")?;
                    out.write_all(t.feature().as_bytes())?;
                    out.write_all(b"\n")?;
                }
                out.write_all(b"EOS\n")?;
                if is_tty {
                    out.flush()?;
                }
            }
            OutputMode::Wakati => {
                for i in 0..worker.num_tokens() {
                    if i != 0 {
                        out.write_all(b" ")?;
                    }
                    out.write_all(worker.token(i).surface().as_bytes())?;
                }
                out.write_all(b"\n")?;
                if is_tty {
                    out.flush()?;
                }
            }
            OutputMode::Detail => {
                for i in 0..worker.num_tokens() {
                    let t = worker.token(i);
                    writeln!(
                        &mut out,
                        "{}\t{}\tlex_type={:?}\tleft_id={}\tright_id={}\tword_cost={}\ttotal_cost={}",
                        t.surface(),
                        t.feature(),
                        t.lex_type(),
                        t.left_id(),
                        t.right_id(),
                        t.word_cost(),
                        t.total_cost(),
                    )?;
                }
                out.write_all(b"EOS\n")?;
                if is_tty {
                    out.flush()?;
                }
            }
        }
    }

    Ok(())
}
